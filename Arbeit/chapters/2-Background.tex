\chapter{Theoretical background}
\label{Background}

In this chapter, we are going to introduce the reader to the theoretical concepts, which represent a prerequisite to have the understanding of this thesis.

\section{Main concepts}
\label{Background-Main}

\textit{Distributed database} is ``a collection of multiple, logically interrelated databases distributed over a computer network''\cite{11}. A \textit{geo-distributed database}, in its turn, is a database, which is spread across two or more geographically distinct locations and runs without experiencing performance delays. The maintaining of such databases brings its challenges. As the database is spread across several locations, there should be a replication process in order to ensure that replicas of that database synchonize and have the latest state of the data. This replication process should be fast, because if there are two replicas of the database, then whenever there is some information written to the first replica, it should be accessable to users, who use the second one. That is the problem of the availability, but before the information at replicas becomes available, it first should be checked over the consistency, as the states of the replicas should be equal. That is a complex task to solve. 

Working with such a distributed database, whenever the data is needed to be read or changed in any way, a transaction should be started, executed and closed. A \textit{transaction} is a basic unit of computing, which consists of sequence of operations that are applied atomically on a database. Transactions transform a consistent database state to another consistent database state. A transaction is considered to be \textit{correct}, if it obeys the rules, specified on the database. As long as each transaction is correct, a database guarantees that concurrent execution of user transactions will not violate database consistency \cite{11}. \textit{Consistency} requires transactions to change the data only according to the specified rules. An example of consistency rule can be the following: let us say that in a bank database the bank account number should only consist of integer numbers. If an employee tries to create an account that contains something other than integer numbers in it, then the database consistency rule will disallow it. Consistency rules are important as they control the incoming data and reject the information, which does not fit.

\textit{Sequential consistency} and \textit{linearizability} are two consistency conditions that are well-known. Sequential consistency requires that all of the data operations appear to have executed atomically (i.e. independently), in some sequential order. When this order must also preserve the global ordering of non-overlapping operations, this consistency is called linearizability\cite{27}. Linearizability guarantees that the same operations are applied in the same order to every replica of the data item\cite{12}. Serializability is a guarantee about transactions, that they are executed serially (i.e. without overlapping in time) on every set of the data items\cite{12}. Serializability is more strict than sequential consistency, as the granularity of sequential consistency is a single operation, while for serializability it is a transaction. As a result, when serializability is satisfied, the sequential consistency is also satisfied, but not vice versa.

Now, let us introduce different consistency models and the one we will follow in the designing part of WebCure. 

As stated by \citet{10}, ``\textit{strong consistency} model could be described in the following way: whenever the update is performed, everyone knows about it''. It means that there is a total order of updates and reads are guaranteed to return the latest data, irregardless of which replica is the source of the response. The advantage of strong consistency is that the database is always in a consistent state and to disadvantages we can add low latency, as there is a delay for making sure that all the replicas are in a consistent state before any other read / write requests could be processed. The latency point is a huge drawback for the performance, especially if strong consistency model is considered to be used as a solution for the web, where users usually expect high responsiveness and availability. 

The main point of replicating data is to improve such aspects as reliability, availability, performance and latency. However, according to CAP Theorem\cite{29}, a distributed database can only have two of the three properties: consistency, availability and partition tolerance. This theorem is very important, as it makes people think towards the trade-off between those three properties for a specific use case. There are some weaker consistency models, where the results of requests can alter depending on the replica\cite{28}.  In this thesis, we will stick with partial \textit{causal consistency}. As it is stated in \citet{7}, causal consistency is ``the strongest available and convergent model''. They continue their statement saying that ``under causal consistency, every process observes a monotonically non-decreasing set of updates that includes its own updates, in an order that respects the causality between operations''. As the causal ordering is respected, it makes it easier for programmers to reason, as it gives the guarantee that related events are visible in the order of occurence, while the events, which have no relation to each other, can be in different order in different replicas. Let us consider an example of an application for some of social netwoks. There, a reply to a wall past happens after the original post is pubslihed. Thus, users should not see the reply before the original post is observable. This type of guarantees are provided by causal consistency. Looking at the \figref*{fig:theory3} we can see that on the left subfigure, the user can see the original wall post as well as the reply, while on the right subfigure, without causal consistency, the user sees only the reply, while the original wall post is missing. 

\begin{figure}%
    \centering
    \def\svgwidth{0.4\linewidth}
    \subfloat[]{{\input{images/causal/causal_true.pdf_tex}}}%
    \qquad
    \def\svgwidth{0.4\linewidth}
    \subfloat[]{{\input{images/causal/causal_false.pdf_tex}}}%
    \caption{An example of how a causally consistent behaviour (a) and the one that is not (b) could work in a social network.}%
    \label{fig:theory3}%
\end{figure}





\section{AntidoteDB}
\label{2-antidotedb}

For this thesis, one of the core parts in the architecture of the WebCure belongs to the database called AntidoteDB\cite{4}. It helps programmers to write correct applications and has the same performance and horizontal scalability as AP / NoSQL\cite{14}, while it also:

\begin{itemize}
\item {is geo-distributed, which means that the datacenters of AntidoteDB could be spread across anywhere in the world;}
\item {groups operations into atomic transactions\cite{9, 15};}
\item {delivers updates in a causal order and merges concurrent operations;}
\end{itemize} 

Merging concurrent operations is possible because of Conflict-Free Replicated Datatypes (CRDTs) \cite{2}, which are used in AntidoteDB. It supports counters, sets, maps, multi-value registers and other types of data that are designed to work correctly in the presence of concurrent updates and failures. The usage of CRDTs allows the programmer to avoid problems that are common for other databases like NoSQL, which are fast and available, but hard to program against\cite{15}. We will cover the topic of CRDTs later in this chapter.

Apart from that, to replicate the data AntidoteDB implements the \textit{Cure}\cite{15} protocol. It is a highly scalable protocol, which provides causal consistency. 

To ensure the guarantees it offers, AntidoteDB uses timestamps, indicating the time after the transaction. Timestamps are considered to be unique, totally ordered, and consistent with causal order, which means that if \textit{operation 1} happened before \textit{operation 2}, then the timestamp related to the \textit{operation 2} is greater than the one related to the \textit{operation 1}\cite{2}.Whenever the update operation has to be applied, it is also possible to provide a minimum time from what that update should be performed. This information is useful, when a client is working with a server, which is based on AntidoteDB. In such cases, when one data center stops working, the client can reconnect to another one. As a client can remember the latest timestamp for the data it has worked on, the failover to another data center is possible without any additional efforts, as the timestamp information will help the client to request only the data, which has timestamps greater than the one, which is already stored at the client.

\section{Conflict-Free Replicated Datatypes (CRDTs)}
\label{2-crdts}

As it is stated in the work of \citet{3}, a conflict-free replicated datatype (CRDT) is an abstract datatype, which is designed for a possibility to be replicated at multiple processes and possesses the following properties:


    \begin{itemize}
        \item {The data at any replica can be modified independently of other replicas}
        \item {Replicas deterministically converge to the same state when they received the same updates}
    \end{itemize}

Replication is a fundamental concept of distributed systems, well studied by the distributed algorithms community\cite{2}. There are two models of replication that are considered: state-based and operation-based. We are going to introduce our reader to both of them below. 

\subsection*{Operation-based replication approach}

\begin{figure}[!htb]
    \begin{center}
    \def\svgwidth{\linewidth}
    \input{images/crdts-replication/op-based.pdf_tex}
    \caption {Operation-based approach\cite{2}. <<S>> stands for source replicas and <<D>> for downstream replicas. }
    \label{fig:theory1}
\end{center}
\end{figure}

In this thesis, we are going to use the operation-based replication approach, where replicas converge by propagating operations to every other replica\cite{3}. Once an operation is received in a replica, it is applied locally. Afterwards, all replicas would possess all of the updates. At the \figref*{fig:theory1}, we can see that firstly operations \textit{f(x\textsubscript{i})}, \textit{g(x\textsubscript{i})} applied locally at source replicas \textit{x\textsubscript{i}}, and then the operations are conveyed to all the other replicas. The second part of this process is named \textit{downstream} execution.

This replication approach infers that replicas do not exchange full states with each other, which is a positive in terms of efficiency. Not always, though, as it depends on the task. Sometimes, applying multiple operations at every replica could be costly and this is where state-based replication approach is beneficial.

% TODO What are the requirements for the op-based CRDTs to work correctly?
% Give an example of an op-based CRDT specification? counter!!!

\subsection*{State-based replication approach}
\begin{figure}[!htb]
    \begin{center}
    \def\svgwidth{\linewidth}
    \input{images/crdts-replication/state-based.pdf_tex}
    \caption {State-based approach\cite{2}. <<S>> stands for source replicas and <<M>> for merging stages.}
    \label{fig:theory2}
\end{center}
\end{figure}

The idea of this approach is kind of opposite to the operation-based one. Here, every replica, when it receives an update, first applies it locally. Afterwards, it sends its updated state to other replicas. Following this way, every replica sends its current full state to other replicas. Afterwards, the merge function is applied between a local state and a received state, and every update eventually is going to appear at every other replica in the system. We can see this it at the \figref*{fig:theory2}, where \textit{x\textsubscript{i}} stands for replicas, \textit{f(x\textsubscript{i})}, \textit{g(x\textsubscript{i})} stands for the functions that apply updates locally at source replicas before sending a new full state for a follow-up merges to other replicas in the system.

%What are the requirements for the state-based CRDTs to work correctly?

There are different types of CRDTs, however, we will consider three of them: counters, sets and multi-value registers. We will give a brief description to each of the mentioned datatypes below.

\subsection*{Counter}

This is a datatype, which keeps track on its state, which is an integer number. The value of the Counter could be modified by the operations \textit{inc} and \textit{dec} that increases or decreases the state by one unit, accordingly\cite{3}. The concurrency semantics for this datatype is that a final state of the object reflects all the performed operations on it. In other words, to calculate the state of the Counter, it is needed to count the number of increments and substract the number of decrements. 

\subsection*{Add-wins Set}

Add-wins Set\footnote{for the simplicity of reading, later it goes as Set} datatype represents a collection of objects with a specific handling of concurrent updates performed over them. In case of concurrent updates on the same object, \textit{add} operations in Set win against \textit{remove} operations. If, for example, there is an empty set \textit{} and two concurrent operations applied to it -- \textit{add a} and \textit{remove a}, then the result is going to be \textit{\{a\}}, as \textit{add} operation wins. A \textit{remove} operation will ``overwrite'' an \textit{add} operation only when it happens after it\cite{3}. 

\subsection*{Multi-value Register}

This datatype maintains a value and provides a \textit{write} operation of updating that value. The interesting part about Multi-value Registers is their concurrency semantics. In case of two or more updates happening at the same time, all values are kept. Thus, the state of the register will consist of all the concurrently written updates for a further processing. However, any additional single \textit{write} operation will overwrite the previous state of the register, even if it consisted of multiple values\cite{39}.